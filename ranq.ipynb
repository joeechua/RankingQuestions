{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step of Front Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding(nn.Module):\n",
    "    def __init__(self, image_channel_type='I', output_size=1024, mode='train',\n",
    "                 extract_features=False, features_dir=None):\n",
    "        super(ImageEmbedding, self).__init__()\n",
    "        self.extractor = models.resnet50(pretrained=True)\n",
    "        # freeze feature extractor (ResNet50) parameters\n",
    "        for param in self.extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        extactor_fc_layers = list(self.extractor.children())[:-1]\n",
    "        if image_channel_type.lower() == 'normi':\n",
    "            extactor_fc_layers.append(Normalize(p=2))\n",
    "        self.extractor.classifier = nn.Sequential(*extactor_fc_layers)\n",
    "\n",
    "        self.fflayer = nn.Sequential(\n",
    "            nn.Linear(1000, output_size),\n",
    "            nn.Tanh())\n",
    "\n",
    "        # TODO: Get rid of this hack\n",
    "        self.mode = mode\n",
    "        self.extract_features = extract_features\n",
    "        self.features_dir = features_dir\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Pdb().set_trace()\n",
    "        if not self.extract_features:\n",
    "            image = self.extractor(image)\n",
    "            # if self.features_dir is not None:\n",
    "            #     utils.save_image_features(image, image_ids, self.features_dir)\n",
    "        \n",
    "        image_embedding = self.fflayer(image)\n",
    "        return image_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Embedding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From senior's code (detect.py)\n",
    "image = cv2.imread(\"images/joTest3.png\")\n",
    "#bg_img = cv2.resize(image, (512, 512))\n",
    "orig_image = image.copy()\n",
    "# # BGR to RGB\n",
    "image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "# make the pixel range between 0 and 1\n",
    "image /= 255.0\n",
    "# bring color channels to front\n",
    "image = np.transpose(image, (2, 0, 1)).astype(np.float64)\n",
    "# # convert to tensor\n",
    "image = torch.tensor(image, dtype=torch.float)\n",
    "# # add batch dimension\n",
    "image = image.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = ImageEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = embed(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptor Embeddings\n",
    "sBert for embeddings, need to use the preprocessors to get the descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/stsb-roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10/170489.png File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = [\"I should buy a Butterfinger because it will make my life more fun. \", \"I should eat butterfingers because the simpsoms say\", \"I should buy this product because it will make me as funny as Bart Simpson.\"]\n",
    "sentiment = [[\"6\", \"14\"], [\"12\"], [\"11\"]]\n",
    "strat = [[\"5\"], [\"6\"], [\"2\", \"5\"], [\"2\", \"6\", \"8\"], [\"5\", \"6\", \"9\"]]\n",
    "topic = [\"2\", \"2\", \"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(target_lst):\n",
    "    \"\"\"\n",
    "    Transform the target_lst of sentiments provided by the PITTs dataset to\n",
    "    a Pytorch tensor based on the Word2Vec model.\n",
    "\n",
    "    target_list: a list of lists where each element is a number\n",
    "    \"\"\"\n",
    "    # flatten list\n",
    "    lst = [item for sublist in target_lst for item in sublist]\n",
    "\n",
    "    # convert to int\n",
    "    lst = [int(num) for num in lst]\n",
    "\n",
    "    return max(lst, key=lst.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get from preprocessors, I think need to do all this inside Ads Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(sentiment) #amazed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(topic) #chocolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(strat) #culture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings for each  of the descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = torch.tensor(model.encode(\"amazed\"))\n",
    "stra = torch.tensor(model.encode(\"culture\"))\n",
    "top = torch.tensor(model.encode(\"chocolate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = torch.tensor(model.encode(qa[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024/4 #because I want to concatenate 4 qa combinations to have the size of 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_proc = nn.Sequential(nn.Linear(768,256),nn.Tanh()) #to make the 300-d word embeddings into 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_small = qa_proc(first) #get the 256-d word embedding for the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stra = torch.mul(first, stra) #multiply qa and strategy\n",
    "qa_stra = qa_proc(qa_stra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_top = torch.mul(first, top) #multiply qa and topic\n",
    "qa_top = qa_proc(qa_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_sent = torch.mul(first, sent) #multiply qa and sentiment\n",
    "qa_sent = qa_proc(qa_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_res = torch.cat([qa_sent, qa_small, qa_top, qa_stra], dim=0) #concatenate the 4 256-d embeddings to become 1024-d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutan Fusion & MLP from VQA\n",
    "For Lydia to see how the mlp output will be like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutanFusion(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, num_layers):\n",
    "        super(MutanFusion, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        hv = []\n",
    "        for i in range(self.num_layers):\n",
    "            do = nn.Dropout(p=0.5)\n",
    "            lin = nn.Linear(input_dim, out_dim)\n",
    "\n",
    "            hv.append(nn.Sequential(do, lin, nn.Tanh()))\n",
    "        #\n",
    "        self.image_transformation_layers = nn.ModuleList(hv)\n",
    "        #\n",
    "        hq = []\n",
    "        for i in range(self.num_layers):\n",
    "            do = nn.Dropout(p=0.5)\n",
    "            lin = nn.Linear(input_dim, out_dim)\n",
    "            hq.append(nn.Sequential(do, lin, nn.Tanh()))\n",
    "        #\n",
    "        self.ques_transformation_layers = nn.ModuleList(hq)\n",
    "\n",
    "    def forward(self, ques_emb, img_emb):\n",
    "        # Pdb().set_trace()\n",
    "        batch_size = img_emb.size()[0]\n",
    "        x_mm = []\n",
    "        for i in range(self.num_layers):\n",
    "            x_hv = img_emb\n",
    "            x_hv = self.image_transformation_layers[i](x_hv)\n",
    "\n",
    "            x_hq = ques_emb\n",
    "            x_hq = self.ques_transformation_layers[i](x_hq)\n",
    "            x_mm.append(torch.mul(x_hq, x_hv))\n",
    "        #\n",
    "        x_mm = torch.stack(x_mm, dim=1)\n",
    "        x_mm = x_mm.sum(1).view(batch_size, self.out_dim)\n",
    "        x_mm = F.tanh(x_mm)\n",
    "        return x_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutan = MutanFusion(1024, 1024, 5)\n",
    "mlp = nn.Sequential(nn.Linear(1024, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Userr\\anaconda3\\envs\\FIT3163Project\\lib\\site-packages\\torch\\nn\\functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "combined = mutan(qa_res, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = mlp(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, preds = torch.max(final, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "check1 = torch.tensor([1,1,1,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  True])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds == check1).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((preds == check1).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check1.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.2206], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([15]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(final, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Userr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from dataset import VQADataset\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = VQADataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_dataset(dataset: VQADataset):\n",
    "    \"\"\"Split the dataset into training and testing\n",
    "\n",
    "    Args:\n",
    "        dataset (AdsDataset): a Pytorch Dataset\n",
    "\n",
    "    Returns:\n",
    "        (AdsDataset, AdsDataset): train dataset, test dataset\n",
    "    \"\"\"\n",
    "    # randomly select the training and testing indices\n",
    "    indices = list(range(len(dataset.info_path)))\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        indices, train_size=0.85, shuffle=True, random_state=24)\n",
    "\n",
    "    # split the dataset into train and test\n",
    "    train_dataset = torch.utils.data.Subset(VQADataset(transforms=get_transform(train=True)), train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(VQADataset(transforms=get_transform(train=False)), test_indices)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import get_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = create_train_test_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [6,3,3]\n",
    "max(lst, key=lst.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Userr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from preprocess.descriptors import SentimentPreProcessor, StrategiesPreProcessor, TopicsPreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_proc = SentimentPreProcessor()\n",
    "strat_proc = StrategiesPreProcessor()\n",
    "top_proc = TopicsPreProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_transform(target_lst):\n",
    "    \"\"\"\n",
    "    Transform the target_lst of sentiments provided by the PITTs dataset to\n",
    "    a Pytorch tensor based on the Word2Vec model.\n",
    "\n",
    "    target_list: a list of lists where each element is a number\n",
    "    \"\"\"\n",
    "    # flatten list\n",
    "    lst = [item for sublist in target_lst for item in sublist]\n",
    "\n",
    "    # convert to int\n",
    "    lst = [int(num) for num in lst]\n",
    "\n",
    "    return max(lst, key=lst.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.descriptors import cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(target_lst, descriptor):\n",
    "    \"\"\"\n",
    "    Transform the target_lst of topics provided by the PITTs dataset to\n",
    "    a Pytorch tensor based on the sBERT model.\n",
    "\n",
    "    target_list: a list of lists, each element may contain a number or text\n",
    "    \"\"\"\n",
    "\n",
    "    # flatten list\n",
    "    target_lst = [item for sublist in target_lst for item in sublist]\n",
    "\n",
    "    count = 0\n",
    "    vec_lst = []\n",
    "    num_lst = []\n",
    "\n",
    "    if descriptor == \"sentiment\":\n",
    "        proc = sent_proc\n",
    "    elif descriptor == \"topic\":\n",
    "        proc = top_proc\n",
    "    else:\n",
    "        proc = strat_proc\n",
    "\n",
    "    for el in target_lst:\n",
    "        try:\n",
    "            x = int(el)\n",
    "            num_lst.append(x)\n",
    "            count += 1\n",
    "        except ValueError:\n",
    "            # Get the vector representation of this phrase\n",
    "            vec_lst.append(proc.text_embed_model.get_vector_rep(el))\n",
    "\n",
    "    if count == 0:\n",
    "        # The target list has all user text inputs so try to find the\n",
    "        # most represented phrase\n",
    "        cosines = [0] * len(vec_lst)\n",
    "        for i in range(len(vec_lst)):\n",
    "            for j in range(len(vec_lst)):\n",
    "                if i != j:\n",
    "                    cosines[i] += cosine_sim(vec_lst[i], vec_lst[j])\n",
    "\n",
    "        max_val = max(cosines)\n",
    "        max_index = cosines.index(max_val)\n",
    "\n",
    "        final = target_lst[max_index]\n",
    "\n",
    "    else:\n",
    "        if 0 in num_lst:\n",
    "            num_lst.remove(0)\n",
    "        final = proc.id_to_word[max(num_lst, key=num_lst.count)]\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_proc.id_to_word.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOES NOT EXIST hilariouss\n",
      "DOES NOT EXIST 12oz\n",
      "DOES NOT EXIST threat/violence\n",
      "DOES NOT EXIST textee\n",
      "DOES NOT EXIST 02/03/2015\n",
      "DOES NOT EXIST die/reincarnate\n",
      "DOES NOT EXIST iceburg\n",
      "DOES NOT EXIST chimmney\n"
     ]
    }
   ],
   "source": [
    "main_idx = 1\n",
    "for image_id, qa, sentiments, strategies, topics, slogans in ds:\n",
    "    #get top id for sentiments, strategies, topics\n",
    "    sentiment = transform(sentiments, \"sentiment\")\n",
    "    strategy = transform(strategies, \"strategy\")\n",
    "    topic = transform(topics, \"topic\")\n",
    "    #combine best s,s,t with each combination of qa and slogan\n",
    "    for question in qa:\n",
    "        for slogan in slogans:\n",
    "            all_data[main_idx] = {\"Slogan id\": main_idx, \"Image\": image_id, \"Sentiment\": sentiment, \"Strategy\": strategy, \"Topic\":topic, \"QA\": question, \"Slogan\": slogan}\n",
    "            main_idx += 1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = json.dumps(all_data)\n",
    "with open(\"slogan_descriptor_combos.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1084"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.indices[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.6353, 0.6392, 0.6392,  ..., 0.6275, 0.6314, 0.6314],\n",
       "          [0.6392, 0.6392, 0.6353,  ..., 0.6275, 0.6275, 0.6314],\n",
       "          [0.6353, 0.6392, 0.6392,  ..., 0.6275, 0.6275, 0.6275],\n",
       "          ...,\n",
       "          [0.6784, 0.6784, 0.6784,  ..., 0.6431, 0.6392, 0.6314],\n",
       "          [0.6784, 0.6824, 0.6824,  ..., 0.6431, 0.6431, 0.6431],\n",
       "          [0.9294, 0.9373, 0.9294,  ..., 0.9294, 0.9216, 0.9294]],\n",
       " \n",
       "         [[0.7529, 0.7569, 0.7569,  ..., 0.7451, 0.7490, 0.7490],\n",
       "          [0.7569, 0.7569, 0.7529,  ..., 0.7451, 0.7451, 0.7490],\n",
       "          [0.7529, 0.7569, 0.7569,  ..., 0.7451, 0.7451, 0.7451],\n",
       "          ...,\n",
       "          [0.8039, 0.8000, 0.8039,  ..., 0.7608, 0.7608, 0.7529],\n",
       "          [0.7882, 0.7882, 0.7922,  ..., 0.7490, 0.7569, 0.7490],\n",
       "          [0.9647, 0.9647, 0.9647,  ..., 0.9569, 0.9569, 0.9569]],\n",
       " \n",
       "         [[0.7922, 0.7961, 0.7961,  ..., 0.7843, 0.7882, 0.7882],\n",
       "          [0.7961, 0.7961, 0.7922,  ..., 0.7843, 0.7843, 0.7882],\n",
       "          [0.7922, 0.7961, 0.7961,  ..., 0.7843, 0.7843, 0.7843],\n",
       "          ...,\n",
       "          [0.8431, 0.8510, 0.8431,  ..., 0.8118, 0.8039, 0.8039],\n",
       "          [0.8196, 0.8235, 0.8235,  ..., 0.7843, 0.7843, 0.7843],\n",
       "          [0.9686, 0.9686, 0.9686,  ..., 0.9608, 0.9608, 0.9608]]]),\n",
       " {'qa': array([-1.02449775e-01, -2.44367979e-02,  2.45102048e-02,  2.33185962e-01,\n",
       "          2.99263988e-02, -2.79600024e-02,  1.79124996e-01, -6.49026036e-02,\n",
       "          2.73579992e-02, -1.02406609e+00, -7.57862106e-02, -4.04265970e-01,\n",
       "          1.66120045e-02,  3.29412013e-01, -8.38816017e-02,  1.06610797e-01,\n",
       "         -1.28834397e-01,  5.26289940e-02, -1.77777410e-02,  1.57311797e-01,\n",
       "          1.51348010e-01,  2.06275582e-01, -7.76939914e-02,  2.33449012e-01,\n",
       "         -1.68771803e-01,  1.55617997e-01, -1.54289603e-03,  3.15798037e-02,\n",
       "          6.73145195e-03, -2.48995394e-01, -1.13820195e-01,  2.92083591e-01,\n",
       "         -5.03574014e-02,  1.26718804e-01, -6.69113994e-01,  3.70462000e-01,\n",
       "          4.16945927e-02,  1.78562015e-01, -2.29793981e-01,  1.36890009e-01,\n",
       "         -7.62763992e-02,  1.05512403e-01, -3.54459994e-02,  7.72768036e-02,\n",
       "         -2.21661597e-01,  1.55537203e-01, -2.85542011e-02, -8.05844069e-02,\n",
       "         -9.78455991e-02,  3.00576001e-01, -2.14436017e-02, -1.91542983e-01,\n",
       "         -1.17153205e-01, -3.35555896e-02, -9.88760069e-02, -6.35999721e-03,\n",
       "          6.82400190e-04,  3.26139390e-01,  1.87843651e-01,  7.06668049e-02,\n",
       "         -8.91499501e-03,  2.66070012e-02,  1.43080801e-01,  9.16159973e-02,\n",
       "          2.54386812e-01,  4.99639995e-02, -1.64439976e-02, -5.91726005e-02,\n",
       "          8.96148011e-02,  1.08439978e-02,  2.27269202e-01,  9.08680260e-03,\n",
       "         -7.90844038e-02,  2.66858011e-01,  8.40840042e-02,  3.80679965e-03,\n",
       "          5.97346053e-02, -4.79575992e-02, -2.84032617e-02,  2.02165991e-01,\n",
       "         -2.99985912e-02,  7.60318041e-02,  1.61068603e-01, -7.87900109e-03,\n",
       "         -1.02886394e-01,  1.68340608e-01, -1.32975787e-01,  9.35442001e-02,\n",
       "         -1.97776392e-01, -1.13808200e-01, -5.32620065e-02,  6.97600022e-02,\n",
       "         -5.68223894e-02, -1.32064000e-01,  3.99299972e-02, -1.16753817e-01,\n",
       "         -5.60740232e-02,  1.46643994e-02, -2.16049999e-01, -7.27120191e-02,\n",
       "          5.04000075e-02, -1.77411363e-01, -1.89400313e-03, -2.19733998e-01,\n",
       "         -1.73254013e-01,  1.13075994e-01,  2.17857003e-01,  1.10979602e-01,\n",
       "         -1.25438198e-01,  1.14997979e-02, -4.62973975e-02, -7.04429969e-02,\n",
       "          1.27080008e-01, -2.56614387e-01,  1.62106007e-01,  1.78412586e-01,\n",
       "          4.85488027e-03,  2.47102618e-01,  6.71399981e-02,  4.21035513e-02,\n",
       "         -5.45680039e-02,  7.24800210e-03,  3.12729962e-02,  2.15155989e-01,\n",
       "         -1.31959006e-01, -6.15388043e-02,  7.18878061e-02,  1.89012006e-01,\n",
       "          4.15414050e-02, -6.09429926e-03,  2.41623595e-01,  3.16258013e-01,\n",
       "         -1.89314604e-01,  8.18365589e-02,  1.28107995e-01,  1.76863194e-01,\n",
       "         -6.34434000e-02, -2.03352183e-01, -4.17833962e-02, -4.16998044e-02,\n",
       "          3.34009200e-01, -2.42043808e-01,  1.31376013e-01,  8.28080028e-02,\n",
       "         -4.78468001e-01, -2.98082024e-01, -1.22309979e-02, -7.66771957e-02,\n",
       "         -3.29315990e-01, -7.83261955e-02, -2.25288004e-01,  2.96328843e-01,\n",
       "          9.17429924e-02,  9.20820087e-02,  1.34023190e-01,  9.00594965e-02,\n",
       "         -2.70183980e-01,  1.74664017e-02, -1.51217192e-01, -5.07724062e-02,\n",
       "          2.91152000e-01,  9.86284949e-03,  4.96996343e-02, -6.38635978e-02,\n",
       "         -1.07120797e-01,  9.91271064e-02,  2.07014173e-01,  1.24281999e-02,\n",
       "          5.99085987e-02, -2.59240028e-02, -2.53445387e-01,  1.67410001e-01,\n",
       "         -3.60395372e-01, -7.13887960e-02,  3.54509987e-02, -1.87127993e-01,\n",
       "          7.74974003e-02, -6.96793944e-02,  8.54090005e-02,  2.45259970e-01,\n",
       "          1.74808260e-02,  1.06534004e-01,  1.48357004e-01, -4.57222015e-02,\n",
       "          4.00919914e-02,  1.83548070e-02,  2.27361411e-01,  1.22860000e-01,\n",
       "          3.04781552e-02, -1.95541605e-01, -1.88626796e-01,  2.87247807e-01,\n",
       "          1.58140615e-01,  2.08701994e-02, -1.93646193e-01,  5.74818030e-02,\n",
       "          9.54533964e-02, -1.30934000e-01,  1.62487939e-01,  2.11446002e-01,\n",
       "          8.18720043e-01, -3.43020121e-03,  2.10241407e-01,  2.00034186e-01,\n",
       "         -1.95212007e-01, -1.12917587e-01,  1.67929940e-02,  2.84193462e-04,\n",
       "         -2.01657385e-01, -3.67579982e-02, -8.42383951e-02, -2.61256009e-01,\n",
       "         -2.37230986e-01,  1.41409814e-01, -4.36091945e-02,  1.75695390e-01,\n",
       "          7.75460005e-02, -3.57198715e-03, -9.31018069e-02,  1.02607392e-01,\n",
       "          1.08311616e-01,  6.82599563e-03, -1.72795981e-01, -2.64639616e-01,\n",
       "         -1.66288197e-01, -8.70399028e-02, -3.64406109e-02,  2.32103989e-01,\n",
       "         -1.21538207e-01, -4.71281968e-02,  2.42748022e-01,  2.27985591e-01,\n",
       "          1.15786001e-01, -4.28179987e-02,  2.53441423e-01, -1.26736000e-01,\n",
       "          4.68163975e-02, -6.80352002e-02,  1.08567216e-01, -1.81896806e-01,\n",
       "         -6.17172010e-02, -2.47492820e-01,  3.26498628e-01, -8.69285911e-02,\n",
       "         -2.38132387e-01, -2.17853189e-01, -7.67839998e-02, -1.35876387e-01,\n",
       "         -1.49515003e-01, -4.00088012e-01,  2.00536028e-02,  1.93120003e-01,\n",
       "         -4.70412001e-02,  7.33556002e-02,  2.42210001e-01, -9.89800133e-03,\n",
       "         -5.51890023e-02, -2.97537178e-01,  3.52037400e-01,  4.42980006e-02,\n",
       "         -2.89738983e-01,  4.20560222e-03, -1.10331975e-01, -8.68420117e-03,\n",
       "          1.85974807e-01,  7.50371963e-02, -6.43347949e-02,  2.36709997e-01,\n",
       "          3.64200468e-03,  2.26282161e-02,  1.44910395e-01, -2.31085584e-01,\n",
       "          1.21918797e-01,  1.51217934e-02, -2.32444808e-01,  6.07468002e-02,\n",
       "         -1.20946002e+00,  9.14930031e-02,  1.99755996e-01, -9.11136121e-02,\n",
       "          1.13176204e-01,  1.97747201e-01,  6.88713938e-02,  2.53347605e-01,\n",
       "         -3.09379995e-02, -7.84150138e-02,  1.32919904e-02, -2.07418010e-01,\n",
       "         -3.94821584e-01, -2.55544007e-01, -8.63233954e-02,  2.54315995e-02,\n",
       "          2.55459957e-02, -8.34904015e-02,  1.25200003e-01, -2.96626002e-01,\n",
       "         -9.13880579e-03, -1.05388854e-02, -7.09181949e-02,  3.91722620e-02]),\n",
       "  'sentiment': array([ 2.04990000e-01, -1.56250000e-01,  6.90589994e-02, -2.30639994e-01,\n",
       "          7.21000016e-01, -3.49559993e-01, -6.54259980e-01, -8.57879966e-02,\n",
       "          5.19279973e-04, -9.29859996e-01,  4.41569984e-02,  6.31209984e-02,\n",
       "         -1.90720007e-01,  1.08220004e-01,  5.18469989e-01, -3.20429988e-02,\n",
       "          1.88940004e-01,  2.57869989e-01,  2.30379999e-01,  7.14359999e-01,\n",
       "          2.62279987e-01,  8.63380015e-01, -3.96079987e-01, -4.73230004e-01,\n",
       "         -3.17650009e-03,  2.12009996e-01, -3.29219997e-02,  9.47970003e-02,\n",
       "         -1.73439994e-01,  2.42609993e-01,  1.78870007e-01, -1.74089998e-01,\n",
       "          2.14929998e-01,  1.81150004e-01, -1.00530005e+00,  2.17960000e-01,\n",
       "          7.02220023e-01,  1.25410005e-01,  2.51789987e-01, -5.96369989e-02,\n",
       "          2.06619993e-01,  1.13059998e-01, -1.64729998e-01,  3.39199990e-01,\n",
       "         -5.34250021e-01, -4.39979993e-02,  1.64289996e-01,  2.82160014e-01,\n",
       "          4.20439988e-01, -2.51410007e-02,  4.38050002e-01, -3.54099989e-01,\n",
       "          3.86830002e-01, -3.95810008e-01, -1.80539995e-01,  6.18260019e-02,\n",
       "         -3.86750000e-03,  1.43120006e-01,  1.13339998e-01, -3.24960016e-02,\n",
       "          7.24409968e-02,  2.47339997e-02,  2.32250005e-01,  3.21810007e-01,\n",
       "          4.86790001e-01,  5.14760017e-01, -3.74049991e-01,  1.97559997e-01,\n",
       "         -3.87589991e-01,  4.89879996e-02, -4.32009995e-01, -5.51450014e-01,\n",
       "          9.77210030e-02,  8.79610032e-02, -9.64289978e-02, -1.21830001e-01,\n",
       "          9.20659974e-02,  1.26849994e-01, -6.65199980e-02, -3.88929993e-02,\n",
       "         -5.57770014e-01, -2.46549994e-01, -2.73979992e-01, -4.73289996e-01,\n",
       "          1.55699998e-01,  6.18330017e-02,  4.80800003e-01,  2.76870012e-01,\n",
       "         -3.20959985e-01, -3.52980010e-02, -2.18260005e-01,  1.66830003e-01,\n",
       "         -1.63000003e-01, -8.04599971e-02,  3.21229994e-01, -4.06890005e-01,\n",
       "         -3.85179996e-01, -2.51800001e-01,  6.23430014e-01, -7.54719973e-01,\n",
       "         -5.63120008e-01, -3.68880004e-01, -3.30229998e-01, -2.49610007e-01,\n",
       "         -2.89110005e-01,  3.17220017e-02,  1.06570004e-02,  3.37460011e-01,\n",
       "          1.16650000e-01, -2.79900014e-01,  2.43829995e-01, -7.21689984e-02,\n",
       "          9.13340002e-02,  1.17530003e-01, -4.36670005e-01,  3.15010011e-01,\n",
       "         -2.00900003e-01, -4.86290008e-01, -3.67509991e-01, -1.50999993e-01,\n",
       "          2.43929997e-01,  2.32769996e-01, -1.01379998e-01,  2.35259995e-01,\n",
       "          3.44989985e-01, -5.42530000e-01,  2.01120004e-01,  3.25560011e-02,\n",
       "         -2.77429998e-01,  5.19840010e-02,  6.23459995e-01, -5.75510025e-01,\n",
       "          1.99609995e-01, -4.71240014e-01, -8.47909987e-01,  1.41029999e-01,\n",
       "         -3.17360014e-01, -3.42860013e-01, -8.96760002e-02,  1.03200004e-01,\n",
       "          6.85710013e-02, -1.31750004e-02,  5.73350012e-01, -1.85760006e-01,\n",
       "          1.60689995e-01,  3.22019994e-01, -1.04790004e-02, -3.86099994e-01,\n",
       "         -2.59490013e-01, -3.29589993e-01,  1.15199998e-01,  1.34350002e-01,\n",
       "          3.79700005e-01, -4.86800000e-02,  1.73979998e-01, -1.73400000e-01,\n",
       "         -5.20370007e-01,  8.17650035e-02, -1.60490006e-01, -4.82490003e-01,\n",
       "          3.59679997e-01, -1.30870000e-01,  1.73140004e-01, -4.41760011e-02,\n",
       "          5.33410013e-02, -6.18059993e-01, -4.58320007e-02,  1.55369997e-01,\n",
       "         -3.47550005e-01,  6.64650023e-01,  8.51870000e-01, -1.64749995e-01,\n",
       "         -1.19280005e+00,  1.97400004e-02, -5.93050011e-02,  4.10059988e-01,\n",
       "         -3.16709988e-02, -6.14199996e-01,  2.49510005e-01,  7.63960034e-02,\n",
       "         -3.58119994e-01, -7.45840013e-01,  5.30060008e-02,  1.31400004e-01,\n",
       "          2.33490005e-01,  8.97179991e-02, -2.14870006e-01, -2.68790007e-01,\n",
       "         -3.12449992e-01, -4.95730013e-01, -7.05820024e-02,  2.62210011e-01,\n",
       "          2.12500006e-01,  4.84290004e-01, -7.65210018e-03,  5.26430011e-02,\n",
       "         -1.31789994e+00,  1.97359994e-02, -4.67780009e-02,  6.25069976e-01,\n",
       "          7.64699996e-01,  1.97349992e-02,  2.54700005e-01,  5.65370023e-01,\n",
       "          9.67850015e-02,  4.68129992e-01, -6.68829978e-02,  9.64100003e-01,\n",
       "         -3.30900013e-01, -2.46419996e-01,  5.83360016e-01, -1.87199995e-01,\n",
       "         -7.65509978e-02,  1.32049993e-01, -7.92130008e-02,  4.35770005e-01,\n",
       "         -6.57140017e-02,  1.56399995e-01, -1.91330001e-01,  1.83809996e-01,\n",
       "         -5.61450005e-01,  1.46190003e-01, -5.15290022e-01,  5.83949983e-01,\n",
       "         -3.23729992e-01, -6.76970005e-01,  3.10259998e-01,  6.77429978e-03,\n",
       "          7.19939992e-02,  2.14839995e-01, -4.88720000e-01,  2.33640000e-01,\n",
       "          1.19250000e-01, -4.99709994e-01, -8.61129984e-02,  4.60209996e-01,\n",
       "          1.51249999e-02, -2.49270007e-01, -6.72129989e-01,  1.17849998e-01,\n",
       "          1.79279998e-01,  6.56050026e-01, -3.00669998e-01, -2.42280006e-01,\n",
       "         -1.05879998e+00, -7.04180002e-02, -5.10600023e-03,  3.27670015e-02,\n",
       "          2.91399986e-01,  2.78770000e-01,  4.82459992e-01, -3.67020011e-01,\n",
       "         -7.17490017e-02, -1.83429997e-02, -2.79040009e-01,  1.56599998e-01,\n",
       "         -3.96209992e-02, -1.34859994e-01, -1.57250002e-01,  3.32179993e-01,\n",
       "          2.17759997e-01, -8.35959971e-01,  2.25060001e-01, -3.64360005e-01,\n",
       "         -9.94760022e-02,  3.25219989e-01, -2.88120002e-01,  8.79269987e-02,\n",
       "          1.16820000e-01, -3.93830016e-02,  1.30530000e-01,  1.35759994e-01,\n",
       "          5.13469994e-01, -4.47039992e-01,  1.73549995e-01,  1.27599999e-01,\n",
       "         -6.92170024e-01,  1.75770000e-01, -1.93120003e-01, -2.92210013e-01,\n",
       "         -7.17130005e-01, -2.39519998e-01,  2.58630008e-01, -2.30660006e-01,\n",
       "          1.17080003e-01, -2.08959997e-01, -3.40609998e-01,  7.05280006e-02,\n",
       "         -2.74060011e-01, -7.02780008e-01, -5.46180010e-01, -3.95310014e-01,\n",
       "          2.56550014e-01,  3.70119989e-01,  5.64960003e-01,  3.75880003e-01,\n",
       "         -5.14370024e-01,  3.96149993e-01, -1.69409998e-02, -5.27880013e-01]),\n",
       "  'strategy': array([ 3.68380010e-01, -4.41579998e-01,  5.73419988e-01, -8.57580006e-01,\n",
       "         -6.08370006e-02, -9.79349986e-02, -2.30309993e-01,  4.17589992e-01,\n",
       "          5.03640017e-03, -1.12510002e+00,  1.90300003e-01, -8.03110003e-02,\n",
       "         -6.96950018e-01,  3.96849990e-01,  6.34000003e-01, -4.09950018e-02,\n",
       "          2.69679993e-01,  3.26469988e-02, -5.36260009e-01, -3.66270006e-01,\n",
       "         -4.08050001e-01, -1.43370003e-01, -5.24079986e-02,  2.72229999e-01,\n",
       "          3.25150013e-01, -1.94589999e-02, -1.05269998e-02,  1.36769995e-01,\n",
       "          5.37480004e-02,  3.94659996e-01,  2.65639991e-01, -1.99000001e-01,\n",
       "         -9.51740026e-01,  4.17670012e-01, -1.44769996e-01,  2.75940001e-01,\n",
       "          6.53410032e-02, -4.65240002e-01,  2.68779993e-02,  1.54070005e-01,\n",
       "          3.78030002e-01, -1.44130006e-01, -4.31730002e-01, -2.31840000e-01,\n",
       "         -2.19720006e-01, -4.23830003e-01, -5.19469976e-01,  8.27400014e-02,\n",
       "          2.19650000e-01, -6.19609989e-02,  2.51049995e-01, -2.83010006e-01,\n",
       "          9.27890003e-01, -1.64619997e-01,  1.14540001e-02, -2.60069996e-01,\n",
       "         -1.51840001e-01,  4.87580001e-01,  3.14579993e-01, -4.60919999e-02,\n",
       "          3.83839995e-01,  1.42010003e-02,  2.41449997e-01, -2.70889997e-01,\n",
       "          2.81869993e-02,  2.88100004e-01, -4.93719995e-01,  1.17279999e-01,\n",
       "          6.14560008e-01,  2.82009989e-01, -5.74609995e-01,  3.54310006e-01,\n",
       "         -3.12860012e-01, -1.66380003e-01, -2.04579994e-01, -2.28640005e-01,\n",
       "          5.82669973e-01,  7.19849989e-02,  2.28100009e-02,  3.61229986e-01,\n",
       "         -6.36089981e-01, -7.19969999e-03, -4.25650001e-01, -1.54100001e-01,\n",
       "          2.59460002e-01,  2.47999996e-01, -5.64090014e-01,  3.75990003e-01,\n",
       "         -1.10799998e-01,  4.40130010e-02, -1.23230003e-01, -6.94429994e-01,\n",
       "          1.89400002e-01, -5.37750006e-01, -5.97900003e-02,  5.60179986e-02,\n",
       "          6.03850007e-01, -1.29930004e-01,  3.87840003e-01,  1.80229992e-02,\n",
       "         -1.03990003e-01,  3.63970011e-01,  1.73710007e-02,  3.37130010e-01,\n",
       "          1.04620002e-01,  5.93740009e-02,  6.19340017e-02, -1.62239999e-01,\n",
       "         -2.01689992e-02,  1.41670004e-01,  9.00790021e-02,  7.82790005e-01,\n",
       "          1.21330004e-02, -6.79259980e-03,  8.54619965e-03, -7.52940029e-02,\n",
       "          3.26799989e-01,  3.71960014e-01, -9.37570035e-02, -6.18740022e-01,\n",
       "          2.29269996e-01, -2.66310006e-01,  4.94430006e-01,  5.51670015e-01,\n",
       "         -1.52669996e-01,  2.23780004e-03, -2.28149995e-01,  4.71729994e-01,\n",
       "         -1.82270005e-01,  1.07519999e-01,  6.26349986e-01,  2.32140005e-01,\n",
       "          3.33090007e-01, -3.23969990e-01, -3.81859988e-01, -4.12189998e-02,\n",
       "          2.00039998e-01, -6.52180016e-01,  6.99479997e-01,  1.19700000e-01,\n",
       "         -1.42640003e-03,  3.26310009e-01, -3.49299997e-01,  1.70560002e-01,\n",
       "          3.48619998e-01, -4.52100009e-01, -3.28029990e-01,  1.51629999e-01,\n",
       "         -4.52930003e-01,  1.71120003e-01, -4.20340002e-02, -1.59160003e-01,\n",
       "          2.70500004e-01, -2.56040007e-01,  3.65550011e-01,  3.63840014e-01,\n",
       "          8.29309970e-02, -2.53820002e-01,  1.13300003e-01,  7.42820024e-01,\n",
       "          2.94690013e-01,  4.92449999e-01, -1.02480002e-01,  7.99449980e-02,\n",
       "         -2.25889996e-01,  3.00639987e-01, -3.30969989e-01,  1.86849996e-01,\n",
       "         -5.49640000e-01, -3.21610004e-01, -1.64289996e-01, -5.67179978e-01,\n",
       "         -3.09320003e-01,  4.55879986e-01,  8.28870013e-02, -5.41649997e-01,\n",
       "         -4.81339991e-01, -3.62219989e-01, -4.82290015e-02,  4.93059993e-01,\n",
       "         -4.03750002e-01,  3.27580005e-01,  3.08420002e-01, -8.58969986e-04,\n",
       "          3.18549991e-01, -1.51600003e-01,  3.48269999e-01,  2.48050004e-01,\n",
       "          1.78709999e-01,  1.26619995e-01,  2.79689997e-01, -2.84339994e-01,\n",
       "          1.70029998e-01,  1.23790003e-01, -1.46459997e-01, -9.95600000e-02,\n",
       "         -5.53340018e-02, -2.98700005e-01, -5.21319993e-02,  8.88260007e-02,\n",
       "          1.05949998e+00, -4.42360006e-02,  6.63320005e-01, -1.07989997e-01,\n",
       "          6.45280033e-02, -3.55870008e-01,  9.50749964e-02,  2.35809997e-01,\n",
       "          1.92420006e-01,  8.66069973e-01, -1.67620003e-01, -1.24880001e-01,\n",
       "         -1.03909997e-02, -2.00970005e-02,  2.76490003e-01,  1.67390004e-01,\n",
       "          1.91809997e-01, -9.96910036e-02, -3.28830004e-01,  6.17229998e-01,\n",
       "         -4.57700014e-01,  3.72090012e-01,  6.93300009e-01, -2.20960006e-02,\n",
       "         -2.09849998e-01, -2.87380010e-01, -3.02179992e-01,  9.79819968e-02,\n",
       "          1.87859997e-01,  1.63719997e-01,  8.07619989e-02,  2.15250000e-01,\n",
       "         -1.68009996e-01,  3.29219997e-01, -5.57780027e-01, -2.55149994e-02,\n",
       "          1.25450000e-01, -2.99820006e-01,  1.84790000e-01,  2.50849992e-01,\n",
       "         -8.64030004e-01, -1.01020001e-01,  1.07929997e-01, -4.37189996e-01,\n",
       "         -5.64960018e-02, -1.88079998e-01,  3.82299989e-01, -5.26679993e-01,\n",
       "          2.84960002e-01, -1.55469999e-01, -5.64909987e-02,  9.67440009e-02,\n",
       "         -3.71509999e-01, -7.13609993e-01, -1.05300002e-01, -1.41409993e-01,\n",
       "         -4.17640001e-01, -8.40219975e-01, -9.67779979e-02, -3.83469999e-01,\n",
       "         -4.84329998e-01, -1.53050005e-01,  7.93739974e-01, -1.41709998e-01,\n",
       "         -9.20479968e-02,  3.17250013e-01, -4.01560009e-01,  1.96919993e-01,\n",
       "          4.65669990e-01,  2.18329996e-01, -4.83839989e-01, -4.16520000e-01,\n",
       "          7.26920009e-01, -7.11019993e-01, -5.96650004e-01,  2.09940001e-01,\n",
       "         -1.09710002e+00, -2.94959992e-01,  7.82930017e-01, -2.36589998e-01,\n",
       "         -7.59499967e-02, -4.40050006e-01, -7.10160017e-01, -3.41800004e-01,\n",
       "          8.79329965e-02,  1.83179993e-02, -1.40209999e-02, -1.87149998e-02,\n",
       "          2.78800011e-01, -5.72719984e-02,  4.17420000e-01,  5.30929983e-01,\n",
       "         -2.80149996e-01,  2.32859999e-01,  1.27859995e-01, -4.14889991e-01,\n",
       "          6.03799999e-01, -2.45829999e-01,  1.14600003e-01, -3.30260009e-01]),\n",
       "  'slogan': array([-6.14355020e-02,  1.21786676e-01, -1.77129984e-01,  1.56440496e-01,\n",
       "          1.65385008e-01, -4.07660678e-02,  5.83486669e-02,  4.82549965e-02,\n",
       "         -2.53714025e-01, -7.99246728e-01,  2.24879161e-01, -1.26807675e-01,\n",
       "         -8.11549947e-02, -3.24530095e-01, -6.55891672e-02,  2.32485011e-01,\n",
       "         -4.30941790e-01, -1.63226664e-01,  1.85336664e-01,  2.00380012e-01,\n",
       "         -6.50449917e-02,  1.81875005e-01,  4.49201651e-02, -1.30901664e-01,\n",
       "          2.26218343e-01, -2.50168350e-02, -2.29133200e-03,  2.94299990e-01,\n",
       "         -1.13052100e-01, -1.30783841e-01,  2.18085006e-01,  6.74943253e-02,\n",
       "          3.63658667e-01,  9.47871730e-02, -1.08657753e+00,  1.67316701e-02,\n",
       "         -4.66796421e-02,  1.13962598e-01, -2.00680390e-01,  4.13098335e-01,\n",
       "         -1.17633335e-01,  5.36283106e-03,  3.41551825e-02,  3.34834051e-03,\n",
       "          4.48844999e-01,  1.93415001e-01,  3.88775021e-01, -2.41860166e-01,\n",
       "          4.54798341e-02,  2.83213347e-01, -1.87263176e-01, -1.20122828e-01,\n",
       "          1.04953729e-01, -1.12343334e-01, -6.79800147e-03,  2.43896712e-02,\n",
       "         -1.45084664e-01,  2.32496709e-02,  3.79924886e-02, -1.13380007e-01,\n",
       "          1.28116667e-01,  6.76603541e-02, -6.01399951e-02, -7.01468587e-02,\n",
       "          4.78201592e-03, -1.44212916e-01,  5.20924963e-02, -3.55129987e-01,\n",
       "          1.09945334e-01, -1.08906664e-01,  3.37428540e-01,  2.22993985e-01,\n",
       "         -1.68235004e-02,  1.91926658e-01,  1.72568321e-01, -6.82180002e-02,\n",
       "          3.29132050e-01,  1.97289940e-02,  2.42293999e-01, -3.69522661e-01,\n",
       "          8.16563368e-02,  1.35802492e-01,  1.03761353e-01, -1.10992841e-01,\n",
       "          1.87333319e-02,  9.12486613e-02, -6.56949952e-02,  8.58813226e-02,\n",
       "         -5.82640022e-02, -3.44039686e-02,  7.99927652e-01,  4.11427170e-01,\n",
       "         -2.80128330e-01, -3.02073330e-01,  3.94699946e-02, -1.60894990e-01,\n",
       "         -2.81733334e-01, -6.68329521e-05, -1.44766822e-01, -2.89719164e-01,\n",
       "         -4.58354987e-02,  1.54402003e-01, -1.84508994e-01,  3.14843357e-02,\n",
       "          9.19036642e-02,  1.12547494e-01,  3.90000015e-01, -1.51663318e-01,\n",
       "         -1.14318997e-01,  1.74409643e-01, -1.12227328e-01, -2.91021675e-01,\n",
       "         -9.40691605e-02, -8.51433445e-03,  1.58821002e-01,  7.82200024e-02,\n",
       "         -1.55293003e-01,  3.62957388e-01,  2.28006635e-02, -2.01834992e-01,\n",
       "         -1.35000050e-03, -3.39856654e-01,  2.56978840e-01, -1.94172397e-01,\n",
       "         -1.86482668e-01,  3.20790000e-02,  3.83245498e-01,  9.84446630e-02,\n",
       "         -4.09333408e-03,  8.44683349e-02,  1.39036492e-01,  4.55178529e-01,\n",
       "         -1.64890289e-01,  1.82112336e-01, -8.81634951e-02,  1.18922330e-01,\n",
       "          1.87411651e-01, -1.25814661e-01, -2.90401518e-01,  1.93311334e-01,\n",
       "         -2.91605014e-02,  4.20833416e-02,  8.38719979e-02,  5.82981743e-02,\n",
       "         -5.70626676e-01,  1.75156474e-01, -3.13329339e-01, -1.17844336e-01,\n",
       "          1.34734169e-01,  6.69376627e-02,  2.07505003e-01,  2.49561667e-01,\n",
       "          1.00068331e-01, -1.81561664e-01,  2.98960030e-01, -2.49789998e-01,\n",
       "          4.74799983e-02, -4.83729959e-01,  1.40862167e-01,  5.41179813e-02,\n",
       "          2.93917656e-01, -4.37293798e-01, -1.39559999e-01, -1.86053321e-01,\n",
       "          6.25797436e-02,  1.23079337e-01,  7.26466700e-02,  3.11496675e-01,\n",
       "          1.99125662e-01,  1.41310826e-01, -4.14386660e-01, -7.48918355e-02,\n",
       "         -3.58752638e-01,  1.83434322e-01,  8.84692594e-02,  1.18099945e-02,\n",
       "         -1.57476664e-01,  2.96245009e-01,  8.52459967e-02, -1.93583276e-02,\n",
       "          3.06421667e-01, -9.98866633e-02,  3.95702839e-01,  1.75933003e-01,\n",
       "         -6.77350014e-02, -1.42139167e-01,  8.26139972e-02, -9.60383099e-03,\n",
       "          9.80057120e-02,  2.61787981e-01,  1.89690009e-01,  2.48556674e-01,\n",
       "          8.44402835e-02, -2.57890493e-01,  1.51249349e-01, -2.09341660e-01,\n",
       "         -8.92383233e-02, -5.54309972e-02,  3.67825001e-01, -1.71177819e-01,\n",
       "          1.21908998e+00,  2.04853311e-01,  1.26508147e-01, -1.44571677e-01,\n",
       "         -5.08670025e-02,  2.62977511e-01,  1.74256191e-01,  1.52110994e-01,\n",
       "         -4.20974970e-01,  4.89699878e-02, -2.56468177e-01,  4.95395176e-02,\n",
       "          2.88302302e-02, -1.05393333e-02, -1.15525238e-01, -1.71616673e-01,\n",
       "         -7.76291639e-02,  1.72193334e-01, -5.95163293e-02, -1.05500005e-01,\n",
       "          1.59426391e-01,  2.76660025e-02, -2.75917109e-02, -3.04872334e-01,\n",
       "         -3.24860007e-01, -9.27283242e-02,  3.99921030e-01, -1.74101666e-01,\n",
       "          5.77745028e-02, -4.22133394e-02, -8.81345049e-02,  1.62797987e-01,\n",
       "         -4.28780049e-01,  1.14535004e-01, -2.80332565e-03,  2.51230001e-01,\n",
       "         -3.65483314e-01,  2.81023175e-01, -2.11559981e-02, -1.18325002e-01,\n",
       "          2.44457170e-01, -5.66130318e-03, -1.37455001e-01, -7.87319988e-02,\n",
       "         -2.89179355e-01, -2.15305999e-01, -5.89233339e-02,  8.31140205e-02,\n",
       "          5.27699897e-03,  4.76616621e-02,  1.35968342e-01, -2.93735504e-01,\n",
       "         -2.49019995e-01, -3.34913343e-01,  4.30728525e-01, -2.17448488e-01,\n",
       "         -2.65054971e-01, -3.95730019e-01, -1.35930078e-02, -1.43737659e-01,\n",
       "         -3.81183326e-02, -3.38749975e-01, -1.64135337e-01, -5.19044995e-02,\n",
       "         -1.49941668e-01,  3.48658301e-02, -2.98084974e-01, -1.53798997e-01,\n",
       "         -2.08393689e-02,  2.28068337e-01, -1.01333307e-02, -1.53433159e-01,\n",
       "         -5.32799996e-02,  5.20183332e-02,  1.73248470e-01, -3.42786819e-01,\n",
       "         -1.13872170e+00,  2.55954955e-02, -6.45244360e-01, -1.50519833e-01,\n",
       "         -3.51484865e-01,  1.80544123e-01,  1.81598827e-01,  1.81879833e-01,\n",
       "         -1.34494647e-01, -5.38383424e-02,  1.75755322e-01, -2.67414838e-01,\n",
       "          1.51249915e-02,  1.06136434e-01, -6.74334168e-03, -3.06180984e-01,\n",
       "          3.12093318e-01, -5.66133345e-03,  1.90658674e-01, -1.50441676e-02,\n",
       "          3.76550168e-01, -1.71688318e-01,  1.75316498e-01, -1.45545498e-01]),\n",
       "  'topic': array([-2.03419998e-01,  4.93649989e-01,  2.57280003e-02,  1.03979997e-01,\n",
       "         -8.07479978e-01,  1.19220003e-01, -1.69430003e-02, -1.69850007e-01,\n",
       "          4.39639986e-01, -4.30240005e-01,  4.54959989e-01, -8.28260005e-01,\n",
       "         -3.79869998e-01,  4.18500006e-01, -2.25889996e-01, -1.06370002e-01,\n",
       "          2.95080006e-01, -4.15260009e-02, -1.14390001e-01,  4.25540000e-01,\n",
       "          2.62890011e-01,  1.98410004e-01,  3.34419996e-01,  4.83740002e-01,\n",
       "         -7.35930026e-01, -6.87170029e-02, -6.50900006e-01, -1.44339994e-01,\n",
       "         -1.75840005e-01, -7.83399999e-01, -8.65819991e-01,  6.24599993e-01,\n",
       "         -1.60219997e-01, -4.20240015e-01, -7.13259995e-01,  1.05149996e+00,\n",
       "         -3.24420005e-01,  6.17649972e-01,  1.99250001e-02, -4.17999998e-02,\n",
       "         -8.56169999e-01, -2.49890000e-01,  3.10669988e-01,  1.85859993e-01,\n",
       "          1.35810003e-01, -5.07990003e-01,  2.79170007e-01, -3.56209993e-01,\n",
       "         -4.31040004e-02,  3.48830014e-01,  1.27120003e-01, -6.12439997e-02,\n",
       "          1.51510000e-01,  4.83920008e-01, -2.90690005e-01, -1.03119999e-01,\n",
       "         -3.94410014e-01,  1.92169994e-01,  8.63319993e-01,  3.10039997e-01,\n",
       "         -8.83760005e-02,  4.30929996e-02,  3.25549990e-01, -4.00050014e-01,\n",
       "          6.68649971e-02,  3.51560004e-02,  2.33370006e-01,  2.51329988e-01,\n",
       "         -7.48880029e-01, -2.67019987e-01,  7.14090019e-02,  2.22220004e-01,\n",
       "         -2.23010004e-01,  5.67019992e-02, -6.18209988e-02,  1.69650003e-01,\n",
       "          1.74239993e-01, -4.21640009e-01, -3.15660000e-01, -1.46709993e-01,\n",
       "          3.82849991e-01,  1.54870003e-01, -3.40550005e-01, -4.04949993e-01,\n",
       "          3.55179995e-01, -4.95139986e-01, -1.01930000e-01, -3.16220000e-02,\n",
       "         -4.12200004e-01, -2.98190005e-02, -1.51620001e-01, -2.36829996e-01,\n",
       "         -2.32419997e-01,  1.64570004e-01, -5.28980017e-01,  4.80910003e-01,\n",
       "          3.59459996e-01,  3.16749997e-02, -1.94100007e-01,  1.15070000e-01,\n",
       "          1.09650001e-01,  2.18270004e-01, -9.17169973e-02, -7.67390013e-01,\n",
       "          6.65849969e-02, -7.56910026e-01, -2.29599997e-02,  3.15730006e-01,\n",
       "         -6.08579993e-01,  6.34779990e-01,  6.86160028e-01,  5.04289985e-01,\n",
       "         -2.88769990e-01, -8.10549974e-01,  7.03589991e-02, -9.15630013e-02,\n",
       "         -4.51070011e-01,  7.24619985e-01,  1.61990002e-01, -2.70409994e-02,\n",
       "         -4.40189987e-02,  1.20320000e-01,  2.43249997e-01, -1.26399994e-01,\n",
       "         -3.82539988e-01, -4.97559994e-01,  4.45840001e-01,  7.96540022e-01,\n",
       "         -3.69509995e-01,  3.14990014e-01, -5.43150008e-01,  7.59840012e-01,\n",
       "         -5.33060014e-01,  1.04030001e+00, -4.24419999e-01, -9.44010019e-02,\n",
       "         -5.49979985e-01,  5.04719973e-01, -1.47239998e-01, -5.09800017e-01,\n",
       "          4.72790003e-01, -3.29899997e-01, -5.26820004e-01,  2.44159997e-02,\n",
       "          3.13620001e-01, -4.53399986e-01, -3.07069987e-01, -4.47079986e-01,\n",
       "         -1.00340001e-01, -3.29389989e-01, -6.66410029e-01,  2.59629995e-01,\n",
       "          3.76419991e-01,  2.67859995e-01,  1.55870002e-02, -8.49510014e-01,\n",
       "         -5.64570010e-01, -6.56650007e-01,  1.18950002e-01, -1.80989996e-01,\n",
       "          2.09089994e-01, -2.77839988e-01,  2.70599991e-01, -2.93610007e-01,\n",
       "         -8.62319991e-02,  1.42269999e-01,  4.34179991e-01,  1.76479995e-01,\n",
       "         -1.25479996e-01, -2.48760000e-01,  8.76649991e-02,  1.23389997e-01,\n",
       "         -5.27190007e-02,  3.09789985e-01, -3.56980003e-02, -1.50279999e-01,\n",
       "          7.81389996e-02, -2.78430015e-01,  1.78560004e-01, -6.48599982e-01,\n",
       "         -3.81909996e-01,  2.66629994e-01,  3.93869996e-01, -4.83550012e-01,\n",
       "          2.73250014e-01, -8.07680011e-01,  1.09249997e+00, -3.32920015e-01,\n",
       "          3.54930013e-02, -2.61669993e-01,  5.95830023e-01,  6.20079994e-01,\n",
       "         -2.90270001e-01, -2.48840004e-01, -3.94140005e-01, -2.73589998e-01,\n",
       "         -4.40959990e-01,  1.78570002e-01,  5.14089987e-02,  3.11069995e-01,\n",
       "          4.72880006e-01, -3.00410002e-01,  8.20179999e-01,  3.48450005e-01,\n",
       "         -4.43329990e-01, -4.15080011e-01,  1.65560007e-01,  5.35369992e-01,\n",
       "         -6.03760004e-01, -3.21339995e-01,  2.40410000e-01, -3.68470013e-01,\n",
       "         -2.61130005e-01, -9.38410033e-03, -1.21250004e-01,  6.66140020e-01,\n",
       "          6.33210003e-01, -4.48260009e-01,  5.34669995e-01, -6.80399984e-02,\n",
       "          4.35149997e-01,  5.11179984e-01, -1.29160002e-01, -4.01699990e-02,\n",
       "         -5.45780003e-01, -5.54560013e-02,  9.63450000e-02,  4.42680001e-01,\n",
       "          2.42550001e-01, -1.60620004e-01, -2.71979988e-01, -5.75349987e-01,\n",
       "         -1.46439997e-03, -8.16790015e-03,  6.77510023e-01,  1.72839999e-01,\n",
       "          2.91340007e-03,  2.94349998e-01, -4.42649990e-01, -6.68049991e-01,\n",
       "         -3.01070005e-01, -1.59419999e-01, -8.42109993e-02, -5.74800014e-01,\n",
       "         -1.00689995e+00,  4.22890007e-01, -4.16180015e-01,  1.95999995e-01,\n",
       "          4.86160010e-01, -3.73149991e-01,  5.90340018e-01, -1.68050006e-01,\n",
       "          3.58049989e-01,  4.72449988e-01,  1.66899994e-01,  4.07759994e-02,\n",
       "          2.33480006e-01, -4.23150003e-01,  3.39489996e-01,  2.63790011e-01,\n",
       "         -2.29139999e-01, -4.94480014e-01,  2.35799998e-01, -7.20899999e-02,\n",
       "          7.26610003e-03,  6.30899966e-02, -4.89039987e-01,  3.42510015e-01,\n",
       "          4.58409995e-01,  1.00950003e-01, -3.31330001e-01, -2.88590014e-01,\n",
       "         -3.65719981e-02,  2.94649988e-01,  1.75950006e-01,  3.48410010e-01,\n",
       "         -8.14300001e-01, -5.35229981e-01, -1.46980000e+00, -6.77580014e-02,\n",
       "         -1.47220001e-01,  5.89869976e-01, -6.75949991e-01, -6.23839974e-01,\n",
       "         -6.67619985e-03,  7.38170028e-01,  3.98110002e-01, -3.41870010e-01,\n",
       "          9.32900012e-02,  6.46790028e-01, -4.53429997e-01, -2.64340013e-01,\n",
       "         -4.35419992e-04,  3.58139992e-01,  1.74500003e-01, -5.62960029e-01,\n",
       "         -1.00180000e-01, -7.16229975e-02, -1.52439997e-01, -2.34540001e-01]),\n",
       "  'image id': '10/172164.png'},\n",
       " 1084)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.dataset['1084']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_ds.dataset.combos['10']\n",
    "image_id = data[\"Image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Slogan id': 16949,\n",
       " 'Image': '10/172579.png',\n",
       " 'Sentiment': 'creative',\n",
       " 'Strategy': 'Literal',\n",
       " 'Topic': 'restaurant',\n",
       " 'QA': 'I should use Citibank for my financial needs because they are on the cutting edge of online banking and convenient service that works for me.',\n",
       " 'Slogan': 'vI like being myself. Maybe just slimmer, with a few less wrinkles.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.dataset.combos['16949']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target, answer = train_ds.dataset['1084']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = target[\"qa\"]\n",
    "sentiment = target[\"sentiment\"]\n",
    "strategy = target[\"strategy\"]\n",
    "topic = target[\"topic\"]\n",
    "slogan = target[\"slogan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqa import VQAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQAModel(output_size=len(ds.info_path)).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Userr\\anaconda3\\envs\\FIT3163Project\\lib\\site-packages\\torch\\nn\\functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "ans_scores = model(image, torch.from_numpy(qa), torch.from_numpy(sentiment), torch.from_numpy(strategy), torch.from_numpy(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17356/4247451497.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mans_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mans_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslogan_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(ans_scores, 1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(ans_scores, slogan_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From senior's code (detect.py)\n",
    "image = cv2.imread(\"data/\" + image_id)\n",
    "#bg_img = cv2.resize(image, (512, 512))\n",
    "orig_image = image.copy()\n",
    "# # BGR to RGB\n",
    "image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "# make the pixel range between 0 and 1\n",
    "image /= 255.0\n",
    "# bring color channels to front\n",
    "image = np.transpose(image, (2, 0, 1)).astype(np.float64)\n",
    "# # convert to tensor\n",
    "image = torch.tensor(image, dtype=torch.float)\n",
    "# # add batch dimension\n",
    "image = image.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans_scores = model(image, qa, sentiment, strategy, topic)\n",
    "ans_scores = model(image, torch.from_numpy(qa), torch.from_numpy(sentiment), torch.from_numpy(strategy), torch.from_numpy(topic))\n",
    "_, preds = torch.max(ans_scores, 1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#loss = criterion(ans_scores, slogan_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "slogan_id = torch.tensor([1048])\n",
    "loss = criterion(ans_scores, slogan_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = torch.Tensor([1,2,3,4,5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = criterion(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.432587396908128"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqa import RANQ, ImageEmbedding, QaEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checq = RANQ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emv, desc_emb = checq.forward(image, torch.from_numpy(qa), torch.from_numpy(sentiment), torch.from_numpy(strategy), torch.from_numpy(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Userr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from vqa import MutanFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "muton = MutanFusion(1024, 1024, 5).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_imgh = img_emv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25892/711829242.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmuton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_emv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Userr\\anaconda3\\envs\\FIT3163Project\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Userr\\anaconda3\\RankingQuestions\\vqa.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, ques_emb, img_emb)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mx_hq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mques_emb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mx_hq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mques_transformation_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_hq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mx_mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_hq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_hv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Userr\\anaconda3\\envs\\FIT3163Project\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Userr\\anaconda3\\envs\\FIT3163Project\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Userr\\anaconda3\\envs\\FIT3163Project\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Userr\\anaconda3\\envs\\FIT3163Project\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "muton(img_emv, desc_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('FIT3163Project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d4d26c25ec2ba8b926e8fee0b5ae38d92b9e691e9b7a85d05ab00c091ce94aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
