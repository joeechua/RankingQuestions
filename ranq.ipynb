{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step of Front Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqa import ImageEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Embedding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From senior's code (detect.py)\n",
    "image = cv2.imread(\"images/joTest3.png\")\n",
    "#bg_img = cv2.resize(image, (512, 512))\n",
    "orig_image = image.copy()\n",
    "# # BGR to RGB\n",
    "image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "# make the pixel range between 0 and 1\n",
    "image /= 255.0\n",
    "# bring color channels to front\n",
    "image = np.transpose(image, (2, 0, 1)).astype(np.float64)\n",
    "# # convert to tensor\n",
    "image = torch.tensor(image, dtype=torch.float)\n",
    "# # add batch dimension\n",
    "image = image.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = ImageEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = embed(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptor Embeddings\n",
    "sBert for embeddings, need to use the preprocessors to get the descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/average_word_embeddings_glove.6B.300d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10/170489.png File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = [\"I should buy a Butterfinger because it will make my life more fun. \", \"I should eat butterfingers because the simpsoms say\", \"I should buy this product because it will make me as funny as Bart Simpson.\"]\n",
    "sentiment = [[\"6\", \"14\"], [\"12\"], [\"11\"]]\n",
    "strat = [[\"5\"], [\"6\"], [\"2\", \"5\"], [\"2\", \"6\", \"8\"], [\"5\", \"6\", \"9\"]]\n",
    "topic = [\"2\", \"2\", \"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(target_lst):\n",
    "    \"\"\"\n",
    "    Transform the target_lst of sentiments provided by the PITTs dataset to\n",
    "    a Pytorch tensor based on the Word2Vec model.\n",
    "\n",
    "    target_list: a list of lists where each element is a number\n",
    "    \"\"\"\n",
    "    # flatten list\n",
    "    lst = [item for sublist in target_lst for item in sublist]\n",
    "\n",
    "    # convert to int\n",
    "    lst = [int(num) for num in lst]\n",
    "\n",
    "    return max(lst, key=lst.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(sentiment) #amazed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(topic) #chocolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(strat) #culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = torch.tensor(model.encode(\"amazed\"))\n",
    "stra = torch.tensor(model.encode(\"culture\"))\n",
    "top = torch.tensor(model.encode(\"chocolate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = torch.tensor(model.encode(qa[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_proc = nn.Sequential(nn.Linear(300,256),nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_small = qa_proc(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stra = torch.mul(first, stra)\n",
    "qa_stra = qa_proc(qa_stra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_top = torch.mul(first, top)\n",
    "qa_top = qa_proc(qa_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_sent = torch.mul(first, sent)\n",
    "qa_sent = qa_proc(qa_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_res = torch.cat([qa_sent, qa_small, qa_top, qa_stra], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutan Fusion & MLP from VQA\n",
    "For Lydia to see how the mlp output will be like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqa import MutanFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutan = MutanFusion(1024, 1024, 5)\n",
    "mlp = nn.Sequential(nn.Linear(1024, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Userr\\anaconda3\\envs\\FIT3163Project\\lib\\site-packages\\torch\\nn\\functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "combined = mutan(qa_res, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = mlp(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('FIT3163Project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d4d26c25ec2ba8b926e8fee0b5ae38d92b9e691e9b7a85d05ab00c091ce94aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
